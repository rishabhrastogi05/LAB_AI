{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0-CZxVv51_X"
      },
      "source": [
        "### **Checking for tf-agents installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0spTqDblu-U",
        "outputId": "b61c1ef2-19e0-4a23-9db8-098e1ebc66c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.8/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.1.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqyqefblHsE"
      },
      "source": [
        "### **Importing the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "hn--xj6mlEGU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import abc\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.bandits.environments.bandit_py_environment import BanditPyEnvironment\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erEo2lUl2Vuf"
      },
      "source": [
        "### **Simple Environment :**\n",
        "\n",
        "- **a)**  which  the  observation  is  a  random  integer  between -5  and  5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.\n",
        "\n",
        "- **b)** Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.\n",
        "\n",
        "- **c)** Request  for  50  observations  from  the  environment,  compute  and  print  the total reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYGSvTZL3AeQ"
      },
      "source": [
        "> **Creating the environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "aJHK-zR5lMnc"
      },
      "outputs": [],
      "source": [
        "class CustomBanditEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    super(CustomBanditEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-5, 6, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ9OWKYyp1he",
        "outputId": "ff78bd26-6347-4f5a-83b4-05ce3af914f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "observation: -2\n",
            "action: 2\n",
            "reward: -4.000000\n"
          ]
        }
      ],
      "source": [
        "environment = CustomBanditEnvironment()\n",
        "observation = environment.reset().observation\n",
        "print(\"observation: %d\" % observation)\n",
        "\n",
        "action = 2\n",
        "\n",
        "print(\"action: %d\" % action)\n",
        "reward = environment.step(action).reward\n",
        "print(\"reward: %f\" % reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcXaF0k53SYO"
      },
      "source": [
        "> **Wrapping the environment into a TensorFlow environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "KwOzYQrrr_cM"
      },
      "outputs": [],
      "source": [
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11eWSb0E3aOl"
      },
      "source": [
        "> **Designing an optimal policy manually**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "R3zPAa3twzB5"
      },
      "outputs": [],
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-5, maximum=5)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgvDsZ-g3iKq"
      },
      "source": [
        "> **Computing Total Rewards based on 50 observations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olpuXoErw-nc",
        "outputId": "0e027bf7-67cd-4ce5-9d9e-7498dc822cf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "Observation : 5\n",
            "Action: 2\n",
            "\n",
            "Total Reward : \n",
            "42.0\n"
          ]
        }
      ],
      "source": [
        "sign_policy = SignPolicy()\n",
        "\n",
        "current_time_step = tf_environment.reset()\n",
        "reward = []\n",
        "for _ in range(50):\n",
        "  \n",
        "  print ('Observation : %d' % current_time_step.observation)\n",
        "  action = sign_policy.action(current_time_step).action\n",
        "  print('Action: %d' % action)\n",
        "  reward.append((tf_environment.step(action).reward).numpy()[0][0])\n",
        "\n",
        "print('\\nTotal Reward : ')\n",
        "print(np.sum(reward))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WtQD4xHO3tfe"
      },
      "source": [
        "### **Complex Environment :**\n",
        "- **a)** Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized.\n",
        "\n",
        "- **b)** Define a policy that detects the behaviorof the underlying environment. There are three situations that the policy needs to handle:\n",
        " - i. The agent has not detected know yet which version of the environment is running.\n",
        "\n",
        " - ii. The  agent  detected  that  the  original  version  of  the  environment  is running.\n",
        "\n",
        " - iii. The  agent  detected  that  the  flipped  version  of  the  environment  is running.\n",
        "\n",
        "- **c)** Define the agent that detects the sign of the environment and sets the policy appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTrCKyhS4R3n"
      },
      "source": [
        ">**Creating the environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TAKkBfTw_Qh",
        "outputId": "020a7e94-f677-4e3d-cdcf-c8f046eee4b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reward sign:\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
        "\n",
        "    # Flipping the sign with probability 1/2.\n",
        "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
        "    print(\"reward sign:\")\n",
        "    print(self._reward_sign)\n",
        "\n",
        "    super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign * action * self._observation[0]\n",
        "\n",
        "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsM7__AP4tjI"
      },
      "source": [
        "> **Defining a policy that detects the underlying behaviour of the environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "4fuRUMOe1hzZ"
      },
      "outputs": [],
      "source": [
        "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self, situation):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._situation = situation\n",
        "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                           action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
        "    def case_unknown_fn():\n",
        "      # Choose 1 so that we get information on the sign.\n",
        "      return tf.constant(1, shape=(1,))\n",
        "\n",
        "    # Choose 0 or 2, depending on the situation and the sign of the observation.\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign + 1, shape=(1,))\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1 - sign, shape=(1,))\n",
        "\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPjlZdNV46uB"
      },
      "source": [
        "> **Creating the Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "dgIBMtgC1kz0"
      },
      "outputs": [],
      "source": [
        "class SignAgent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = TwoWaySignPolicy(self._situation)\n",
        "    time_step_spec = policy.time_step_spec\n",
        "    action_spec = policy.action_spec\n",
        "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
        "                                    action_spec=action_spec,\n",
        "                                    policy=policy,\n",
        "                                    collect_policy=policy,\n",
        "                                    train_sequence_length=None)\n",
        "\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "\n",
        "    # We only need to change the value of the situation variable if it is\n",
        "    # unknown (0) right now, and we can infer the situation only if the\n",
        "    # observation is not 0.\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
        "                                  tf.not_equal(reward, 0))\n",
        "\n",
        "\n",
        "    def new_situation_fn():\n",
        "      \"\"\"This returns either 1 or 2, depending on the signs.\"\"\"\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,\n",
        "                            new_situation_fn,\n",
        "                            lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())\n",
        "\n",
        "sign_agent = SignAgent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUjZi25d5EwT"
      },
      "source": [
        "> **Defining the Trajectory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "DcDa_iPZ15z0"
      },
      "outputs": [],
      "source": [
        "# We need to add another dimension here because the agent expects the\n",
        "# trajectory of shape [batch_size, time, ...], but in this tutorial we assume\n",
        "# that both batch size and time are 1. Hence all the expand_dims.\n",
        "\n",
        "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
        "  return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0),\n",
        "                               action=tf.expand_dims(action_step.action, 0),\n",
        "                               policy_info=action_step.info,\n",
        "                               reward=tf.expand_dims(final_step.reward, 0),\n",
        "                               discount=tf.expand_dims(final_step.discount, 0),\n",
        "                               step_type=tf.expand_dims(initial_step.step_type, 0),\n",
        "                               next_step_type=tf.expand_dims(final_step.step_type, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sj-yS2s5H7r"
      },
      "source": [
        "> **Training the agent to learn from experience over 50 timesteps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rELDWff91yzy",
        "outputId": "5674218d-0649-4e9d-ddf1-9a0e39087769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n"
          ]
        }
      ],
      "source": [
        "step = two_way_tf_environment.reset()\n",
        "for _ in range(10):\n",
        "  action_step = sign_agent.collect_policy.action(step)\n",
        "  next_step = two_way_tf_environment.step(action_step.action)\n",
        "  experience = trajectory_for_bandit(step, action_step, next_step)\n",
        "  print(experience)\n",
        "  sign_agent.train(experience)\n",
        "  step = next_step"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
